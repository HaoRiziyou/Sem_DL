{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Generator  Hyper-parameters\n",
    "######################################################################################\n",
    "EMB_DIM = 200 # embedding dimension\n",
    "HIDDEN_DIM = 200 # hidden state dimension of lstm cell\n",
    "MAX_SEQ_LENGTH = 17  # max sequence length\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "#########################################################################################\n",
    "TOTAL_BATCH = 2000\n",
    "dataset_path = \"../data/movie/\"\n",
    "emb_dict_file = dataset_path + \"imdb_word.vocab\"\n",
    "\n",
    "# imdb corpus, id is the one hot translation of txt sentences\n",
    "imdb_file_txt = dataset_path + \"imdb/imdb_sentences.txt\"\n",
    "imdb_file_id = dataset_path + \"imdb/imdb_sentences.id\"\n",
    "\n",
    "eval_file = 'save/eval_file.txt'\n",
    "eval_text_file = 'save/eval_text_file.txt'\n",
    "negative_file = 'save/generator_sample.txt'\n",
    "infer_file = 'save/infer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_data(emb_dict_file):\n",
    "    word_dict = {}\n",
    "    word_list = []\n",
    "    item = 0\n",
    "    with open(emb_dict_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # remove /n\n",
    "            word = line.strip()\n",
    "            word_dict[word] = item\n",
    "            item += 1\n",
    "            word_list.append(word)\n",
    "    length = len(word_dict)\n",
    "    print(\"Load embedding success! Num: %d\" % length)\n",
    "    return word_dict, length, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gen_Data_loader():\n",
    "    def __init__(self, batch_size, vocab_dict):\n",
    "        self.batch_size = batch_size\n",
    "        self.token_stream = []\n",
    "        self.vocab_size = 0\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "    def create_batches(self, data_file_list):\n",
    "        \"\"\"make self.token_stream into a integer stream.\"\"\"\n",
    "        self.token_stream = []\n",
    "        print(\"load %s file data..\" % ' '.join(data_file_list))\n",
    "        for data_file in data_file_list:\n",
    "            with open(data_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    # sentence into separate words\n",
    "                    line = line.split()\n",
    "                    parse_line = [int(x) for x in line]\n",
    "                    # making all the words in the dataset as a 1d stream\n",
    "                    self.token_stream.append(parse_line)\n",
    "\n",
    "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
    "        # cut the taken_stream's length exactly equal to num_batch * batch_size\n",
    "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
    "        self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
    "        self.pointer = 0\n",
    "        print(\"      Load %d * %d batches\" % (self.num_batch, self.batch_size))\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"take next batch by self.pointer\"\"\"\n",
    "        ret = self.sequence_batch[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dis_Data_loader():\n",
    "    def __init__(self, batch_size, vocab_dict, max_sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def load_train_data(self, positive_file_list, negative_file_list):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        for positive_file in positive_file_list:\n",
    "            with open(positive_file)as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    line = line.split()\n",
    "                    parse_line = [int(x) for x in line]\n",
    "                    positive_examples.append(parse_line)\n",
    "        for negative_file in negative_file_list:\n",
    "            with open(negative_file)as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    line = line.split()\n",
    "                    parse_line = [int(x) for x in line]\n",
    "                    negative_examples.append(parse_line)\n",
    "\n",
    "        self.sentences = np.array(positive_examples + negative_examples)\n",
    "        self.sentences = self.padding(self.sentences, self.max_sequence_length)\n",
    "\n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "        # Shuffle the data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
    "        self.sentences = self.sentences[shuffle_indices]\n",
    "        self.labels = self.labels[shuffle_indices]\n",
    "\n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"take next batch (sentence, label) by self.pointer\"\"\"\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n",
    "    def padding(self, inputs, max_sequence_length):\n",
    "        batch_size = len(inputs)\n",
    "        inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32)  # == PAD\n",
    "        for i, seq in enumerate(inputs):\n",
    "            for j, element in enumerate(seq):\n",
    "                inputs_batch_major[i, j] = element\n",
    "        return inputs_batch_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding success! Num: 26401\n"
     ]
    }
   ],
   "source": [
    "# load embedding info\n",
    "vocab_dict, vocab_size, vocab_list = load_emb_data(emb_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../data/movie/imdb/imdb_sentences.id file data..\n",
      "      Load 10926 * 64 batches\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "pre_train_data_loader = Gen_Data_loader(BATCH_SIZE, vocab_dict)\n",
    "pre_train_data_loader.create_batches([imdb_file_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../data/movie/imdb/imdb_sentences.id file data..\n",
      "      Load 10926 * 64 batches\n"
     ]
    }
   ],
   "source": [
    "gen_data_loader = Gen_Data_loader(BATCH_SIZE, vocab_dict)\n",
    "gen_data_loader.create_batches([imdb_file_id])\n",
    "\n",
    "dis_data_loader = Dis_Data_loader(BATCH_SIZE, vocab_dict, MAX_SEQ_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
